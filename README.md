# optimize
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/ambv/black)

reimplementing adaptive variants of gradient descent like RMSProp, AdaGrad, Adam

this code is a refactored version of a setup which served for experiments part of the second assignment in the course

_EE-556 Mathematics of Data: From Theory To Computation_ by Prof. Cevher at EPFL
