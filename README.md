# optimize
reimplementing adaptive variants of gradient descent like RMSProp, AdaGrad, Adam
